import torch
import torch.nn as nn
from diffusion.beta_schedule import make_ddpm_schedule
from diffusion.loss import p_losses

class DummyModel(nn.Module):
    """
    í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ê°€ì§œ ëª¨ë¸.
    Inputê³¼ ë™ì¼í•œ Shapeì˜ ëœë¤ í…ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    def __init__(self):
        super().__init__()
        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í•˜ë‚˜ ì¶”ê°€ (Gradient checkìš©)
        self.dummy_param = nn.Parameter(torch.tensor(1.0))

    def forward(self, x, t):
        # x: (B, C, H, W)
        # t: (B,)
        # Output: (B, C, H, W) - Predicted Noise
        return torch.randn_like(x) * self.dummy_param

def test_loss():
    print("ğŸ§ª Testing Loss Function...")
    
    # 1. Setup
    schedule = make_ddpm_schedule(timesteps=1000)
    model = DummyModel()
    B = 4
    x_start = torch.randn(B, 3, 32, 32)
    t = torch.randint(0, 1000, (B,))
    
    # 2. Calculate Loss
    loss = p_losses(model, x_start, t, schedule, loss_type='l2')
    
    print(f"âœ… Loss calculated: {loss.item()}")
    
    # 3. Check Backward (Gradient Flow)
    loss.backward()
    if model.dummy_param.grad is not None:
        print("âœ… Gradient flow check passed.")
    else:
        print("âŒ Gradient flow check failed!")

    # 4. Check L1 Loss
    loss_l1 = p_losses(model, x_start, t, schedule, loss_type='l1')
    print(f"âœ… L1 Loss calculated: {loss_l1.item()}")
    
    print("\nğŸ‰ Loss Function Test Passed!")

if __name__ == "__main__":
    test_loss()
